{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7kKhuqUEQjD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X-Jx4cNUu9I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVr-eyaSSCyF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuF4VE76SOo9"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/diploma'\n",
        "DATASET_DIR = '/dataset-v3'\n",
        "IMGS_DIR = '/imgs'\n",
        "IMGS_PATH = DATA_PATH + DATASET_DIR + IMGS_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxkPTzwHpH-h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_row_v2(row):\n",
        "  devided = row.split(\" --&#!--- \")\n",
        "  target_part = devided[-1]\n",
        "  target = json.loads(target_part)\n",
        "  return target\n",
        "\n",
        "\n",
        "def parse_actions_file(rows):\n",
        "  actions = []\n",
        "  for row in rows:\n",
        "    if not \" --&#!--- \" in row:\n",
        "      continue\n",
        "    actions.append(parse_row_v2(row))\n",
        "  return actions\n",
        "\n",
        "\n",
        "def reformat_actions_into_dict(parsed_rows):\n",
        "  result = {}\n",
        "  for row in parsed_rows:\n",
        "    result[row['t']] = row\n",
        "  return result\n",
        "\n",
        "\n",
        "def episode_actions_dict(episode_number):\n",
        "  with open(DATA_PATH + DATASET_DIR + \"/\" + str(episode_number), \"r\") as f:\n",
        "    raw_data = f.readlines()\n",
        "  parsed_actions = parse_actions_file(raw_data)\n",
        "  actions_dict = reformat_actions_into_dict(parsed_actions)\n",
        "  return actions_dict\n",
        "\n",
        "\n",
        "def episode_images_list(episode_number):\n",
        "  return os.listdir(IMGS_PATH + str(episode_number))\n",
        "\n",
        "\n",
        "def merge_actions_and_images(actions, images_list, images_path):\n",
        "  SIGNS_TO_COMPARE = 11\n",
        "  def to_short_time(t):\n",
        "    return int(str(t)[:SIGNS_TO_COMPARE])\n",
        "\n",
        "  result = {}\n",
        "  actions_count = len(actions)\n",
        "  images_count = len(images_list)\n",
        "  images_short = {}\n",
        "\n",
        "  for i in images_list:\n",
        "    key = to_short_time(i)\n",
        "    if not key in images_short:\n",
        "      images_short[key] = i\n",
        "\n",
        "  for action_time in actions.keys():\n",
        "    short_time = to_short_time(action_time)\n",
        "    if (short_time in images_short):\n",
        "      the_item = actions[action_time]\n",
        "      the_item['img'] = images_path + '/' + images_short[short_time]\n",
        "      result[short_time] = the_item\n",
        "  return result\n",
        "\n",
        "\n",
        "def reduce_frequency_by_step(episode_data, step):\n",
        "  new_episode_data = {}\n",
        "  sorted_keys = sorted(episode_data.keys())\n",
        "  for i in range(step, len(sorted_keys), 1):\n",
        "    new_elem = episode_data[sorted_keys[i-step]].copy()\n",
        "    next_elem = episode_data[sorted_keys[i]]\n",
        "    new_elem['t'] = next_elem['t']\n",
        "    new_elem['j'] = next_elem['j']\n",
        "    new_episode_data[sorted_keys[i - step]] = new_elem\n",
        "  for i in range(-step, 0, 1):\n",
        "    new_elem = episode_data[sorted_keys[i]].copy()\n",
        "    next_elem = episode_data[sorted_keys[-1]]\n",
        "    new_elem['t'] = next_elem['t']\n",
        "    new_elem['j'] = next_elem['j']\n",
        "    new_episode_data[sorted_keys[i]] = new_elem\n",
        "  return new_episode_data\n",
        "\n",
        "\n",
        "def with_action_horizon(reduced, action_horizon):\n",
        "  reduced_keys = sorted(list(reduced.keys()))\n",
        "  len_reduced = len(reduced)\n",
        "  result = {}\n",
        "\n",
        "  for i in range(len_reduced):\n",
        "    to_append_to_current = []\n",
        "    current = reduced[reduced_keys[i]]['j'].copy()\n",
        "\n",
        "    for j in range(1, action_horizon):\n",
        "      to_append = current.copy()\n",
        "      if (i + j) < len_reduced:\n",
        "        to_append = reduced[reduced_keys[i + j]]['j'].copy()\n",
        "\n",
        "      to_append_to_current += to_append.copy()\n",
        "\n",
        "    result[reduced_keys[i]] = {\n",
        "        'j': reduced[reduced_keys[i]]['j'].copy() + to_append_to_current.copy(),\n",
        "        'current': reduced[reduced_keys[i]]['current'].copy(),\n",
        "        't': reduced[reduced_keys[i]]['t'],\n",
        "        'img': reduced[reduced_keys[i]]['img']\n",
        "        }\n",
        "  return result\n",
        "\n",
        "def form_data_for_episode(episode_number, reduction_step, action_horizon):\n",
        "  actions_dict = episode_actions_dict(episode_number)\n",
        "  images_list = episode_images_list(episode_number)\n",
        "  episode_data = merge_actions_and_images(actions_dict, sorted(images_list), IMGS_PATH + str(episode_number))\n",
        "  reduced_data = reduce_frequency_by_step(episode_data, reduction_step)\n",
        "  data_with_action_horizon = with_action_horizon(reduced_data, action_horizon)\n",
        "  return data_with_action_horizon\n",
        "\n",
        "\n",
        "def episodes_data(numbers_of_episodes, reduction_step, action_horizon):\n",
        "  result = {}\n",
        "  for i in numbers_of_episodes:\n",
        "    result.update(form_data_for_episode(i, reduction_step, action_horizon))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSeAjXAmYja-"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data, image_transform, action_transform):\n",
        "        self.data = list(data.values())\n",
        "        self.image_transform = image_transform\n",
        "        self.action_transform = action_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        image = Image.open(sample['img'])\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        action = self.action_transform(sample['j'])\n",
        "        current = self.action_transform(sample['current'])\n",
        "        return image, torch.tensor(action), torch.tensor(current)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        self.simple_layer = nn.Sequential(\n",
        "            nn.BatchNorm2d(num_features=in_channels),\n",
        "            nn.Conv2d(in_channels=in_channels , out_channels=4*growth_rate,\n",
        "                      kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(num_features=4*growth_rate),\n",
        "            nn.Conv2d(in_channels=4*growth_rate, out_channels=growth_rate,\n",
        "                      kernel_size=3, stride=1, padding=1, bias = False),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        xin = x\n",
        "        xout = self.simple_layer(x)\n",
        "        return torch.cat((xin, xout), 1)\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, number_of_dence_layers, growth_rate):\n",
        "\n",
        "        super().__init__()\n",
        "        self.number_of_dence_layers = number_of_dence_layers\n",
        "        self.simple_block = nn.Sequential(\n",
        "            *[DenseLayer(in_channels + growth_rate * i, growth_rate) for i in range(self.number_of_dence_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.simple_block(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, in_channels, compression_factor):\n",
        "\n",
        "        super().__init__()\n",
        "        self.transition_layer = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Conv2d(in_channels, int(in_channels * compression_factor),\n",
        "                      kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transition_layer(x)\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, in_channels, densenet_config, head_n, growth_rate, compression_factor):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        start_channels = 64\n",
        "\n",
        "        self.prepare_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, start_channels,\n",
        "                      kernel_size=7, stride=2, padding=3, bias = False),\n",
        "            nn.BatchNorm2d(start_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        next_in_channels = start_channels\n",
        "        main_layers = []\n",
        "        for i, block_size in enumerate(densenet_config[:-1]):\n",
        "\n",
        "            main_layers.append(DenseBlock(next_in_channels, block_size, growth_rate))\n",
        "            next_in_channels  = int(next_in_channels + growth_rate * block_size)\n",
        "\n",
        "            main_layers.append(TransitionLayer(next_in_channels, compression_factor))\n",
        "            next_in_channels = int(next_in_channels * compression_factor)\n",
        "\n",
        "        self.main_part = nn.Sequential(*main_layers)\n",
        "\n",
        "        last_layers = []\n",
        "        last_layers.append(DenseBlock(next_in_channels, densenet_config[-1], growth_rate))\n",
        "        next_in_channels  = int(next_in_channels +  + growth_rate * densenet_config[-1])\n",
        "        last_layers.append(nn.BatchNorm2d(next_in_channels))\n",
        "        last_layers.append(nn.AdaptiveAvgPool2d(1))\n",
        "        self.last_block = nn.Sequential(*last_layers)\n",
        "\n",
        "        self.head = nn.Linear(next_in_channels, head_n)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prepare_block(x)\n",
        "        x = self.main_part(x)\n",
        "        x = self.last_block(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Sc8RW3D2hj3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CVAE(nn.Module):\n",
        "  def __init__(self, action_horizon):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = 64\n",
        "    self.action_horizon = action_horizon\n",
        "    self.condition_size = 1024\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(7, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.mean_layer = nn.Linear(256, self.hidden_size)\n",
        "    self.logvar_layer = nn.Linear(256, self.hidden_size)\n",
        "\n",
        "    self.feature_exctractor = DenseNet(3, [6, 12, 24, 16], 512, 32, 0.5)\n",
        "    self.joint_projector = nn.Linear(7, 512)\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(self.condition_size + self.hidden_size, self.hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.hidden_size, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024, 7 * self.action_horizon),\n",
        "    )\n",
        "\n",
        "  def reparametrize(self, mu, logvar):\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    random_sample = torch.randn_like(std)\n",
        "    return std * random_sample + mu\n",
        "\n",
        "  def decode(self, z, image, joints):\n",
        "    image_vector = self.feature_exctractor(image)\n",
        "    joint_vector = self.joint_projector(joints)\n",
        "    concat_vector = torch.cat((z, image_vector, joint_vector), dim=-1)\n",
        "    return self.decoder(concat_vector)\n",
        "\n",
        "  def forward(self, x, image, joints):\n",
        "    latent_representation = self.encoder(x)\n",
        "\n",
        "    mu = self.mean_layer(latent_representation)\n",
        "    logvar = self.logvar_layer(latent_representation)\n",
        "\n",
        "    z = self.reparametrize(mu, logvar)\n",
        "\n",
        "    return latent_representation, mu, logvar, self.decode(z, image, joints)\n"
      ],
      "metadata": {
        "id": "jtSHkN4oNPz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_visuomotor_policy(\n",
        "    model, dataset, learning_rate, batch_size, num_of_epochs, device, shuffle,\n",
        "    validation_set, loss_list, val_loss_list):\n",
        "\n",
        "  RECONSTRUCTION_COEF = 0.75\n",
        "  KLD_COEF = 0.25\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.L1Loss(reduction=\"mean\")\n",
        "  dataloader = torch.utils.data.DataLoader(\n",
        "      dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "  validation_loader = torch.utils.data.DataLoader(\n",
        "      dataset=validation_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(num_of_epochs):\n",
        "    reconstruction_loss_sum = 0\n",
        "    batches_count = 0\n",
        "    for data in dataloader:\n",
        "      batches_count += 1\n",
        "      images, labels, currents = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      currents = currents.to(device)\n",
        "\n",
        "      encoded, mu, log_var, decoded = model(labels, images, currents)\n",
        "\n",
        "      reconstruction_loss = criterion(decoded, labels)\n",
        "      kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "      loss = RECONSTRUCTION_COEF * reconstruction_loss + KLD_COEF * kld_loss\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      reconstruction_loss_sum += reconstruction_loss.item()\n",
        "\n",
        "    epoch_loss = reconstruction_loss_sum / batches_count\n",
        "    loss_list.append(epoch_loss)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = 0\n",
        "    with torch.no_grad():\n",
        "      val_criterion = nn.L1Loss(reduction=\"mean\")\n",
        "      val_batches_count = 0\n",
        "      for val_data in validation_loader:\n",
        "        val_batches_count += 1\n",
        "        v_images, v_labels, v_currents = val_data\n",
        "        v_images = v_images.to(device)\n",
        "        v_labels = v_labels.to(device)\n",
        "        v_currents = v_currents.to(device)\n",
        "\n",
        "        v_encoded, v_mu, v_log_var, v_decoded = model(v_labels, v_images, v_currents)\n",
        "\n",
        "        v_reconstruction_loss = criterion(v_decoded, v_labels)\n",
        "\n",
        "        v_loss += v_reconstruction_loss.item()\n",
        "\n",
        "      val_loss = v_loss / val_batches_count\n",
        "      val_loss_list.append(val_loss)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print(f'epoch \\t{epoch}\\t -- train loss: {epoch_loss} -- val loss: {val_loss}')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "VSbyJhH_Jy5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqcEU1uvvx7t"
      },
      "outputs": [],
      "source": [
        "image_transformator = transforms.Compose([\n",
        "    # transforms.Grayscale(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.ColorJitter(0.4, 0.4, 0.0, 0.3),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_image_transformator = transforms.Compose([\n",
        "    # transforms.Grayscale(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def action_transformator(action):\n",
        "  return action\n",
        "  # result = []\n",
        "  # for i in range(len(action)):\n",
        "  #   result.append((action[i] - (-6.284)) / (6.284 - (-6.284)))\n",
        "  # return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION_HORIZON = 1\n",
        "FREQUENCY_REDUCTION = 25"
      ],
      "metadata": {
        "id": "I57GM27ir3x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD3dT8BU3E9k"
      },
      "outputs": [],
      "source": [
        "train_data = episodes_data([21, 23, 24, 25, 26, 27, 30, 31, 32], FREQUENCY_REDUCTION, ACTION_HORIZON)\n",
        "val_data = episodes_data([28, 29], FREQUENCY_REDUCTION, ACTION_HORIZON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYadv_9HaDSY"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomImageDataset(train_data, image_transformator, action_transformator)\n",
        "val_dataset = CustomImageDataset(val_data, test_image_transformator, action_transformator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEbZG1CYqo0_"
      },
      "outputs": [],
      "source": [
        "model = CVAE(ACTION_HORIZON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx3Yk44AbOK8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm0Dk5GJC4vh"
      },
      "outputs": [],
      "source": [
        "train_loss_list = []\n",
        "val_loss_list = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_visuomotor_policy(model, train_dataset, 0.00001,\n",
        "                                batch_size=8, num_of_epochs=50,\n",
        "                                device=device, shuffle=True,\n",
        "                                validation_set=val_dataset,\n",
        "                                loss_list=train_loss_list,\n",
        "                                val_loss_list=val_loss_list)"
      ],
      "metadata": {
        "id": "qGkS4veNTmjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_visuomotor_policy(model, train_dataset, 0.0000001,\n",
        "                                     batch_size=8, num_of_epochs=10,\n",
        "                                     device=device, shuffle=True,\n",
        "                                     validation_set=val_dataset,\n",
        "                                     loss_list=train_loss_list,\n",
        "                                     val_loss_list=val_loss_list)"
      ],
      "metadata": {
        "id": "cNVRJbfrFAD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_visuomotor_policy(model, train_dataset, 0.000000001,\n",
        "                                     batch_size=8, num_of_epochs=10,\n",
        "                                     device=device, shuffle=True,\n",
        "                                     validation_set=val_dataset,\n",
        "                                     loss_list=train_loss_list,\n",
        "                                     val_loss_list=val_loss_list)"
      ],
      "metadata": {
        "id": "DB7A6H_IIe5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SPJBtLVf_YY"
      },
      "outputs": [],
      "source": [
        "TRAINED_MODEL = '/comparation_cvae_v1_1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJBr6EGfOxcw"
      },
      "outputs": [],
      "source": [
        "torch.save(model, DATA_PATH + '/models' + TRAINED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYtXGWc99aSh"
      },
      "source": [
        "**Model testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziJeESlU992j"
      },
      "outputs": [],
      "source": [
        "def test_resnet(model, dataset, device):\n",
        "  criterion = nn.L1Loss(reduction=\"mean\")\n",
        "  dataloader = torch.utils.data.DataLoader(\n",
        "      dataset=dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "  diffs = []\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.eval()\n",
        "  loss_sum = 0\n",
        "  batches_num = 0\n",
        "  for data in dataloader:\n",
        "    batches_num += 1\n",
        "    images, labels, currents = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    currents = currents.to(device)\n",
        "\n",
        "    z = torch.zeros(1, model.hidden_size).to(device)\n",
        "    prediction = model.decode(z, images, currents)\n",
        "\n",
        "    loss = criterion(prediction, labels)\n",
        "\n",
        "    diff = torch.abs(prediction - labels)\n",
        "    diffs.append(diff.max().item())\n",
        "\n",
        "    loss_sum += loss.item()\n",
        "  epoch_loss = loss_sum / batches_num\n",
        "  return epoch_loss, diffs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "sQHLLNHHU3aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDmg9Aj4CEmk"
      },
      "outputs": [],
      "source": [
        "test_data = episodes_data([19, 22], FREQUENCY_REDUCTION, ACTION_HORIZON)\n",
        "test_d = CustomImageDataset(test_data, test_image_transformator, action_transformator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "metric, diffs = test_resnet(model, test_d, device)\n",
        "print('res= ', metric)\n",
        "print(sorted(diffs)[0:5])\n",
        "print(sorted(diffs)[-5:])"
      ],
      "metadata": {
        "id": "RDscrlRgXGIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = episodes_data([22], FREQUENCY_REDUCTION, ACTION_HORIZON)\n",
        "test_d = CustomImageDataset(test_data, test_image_transformator, action_transformator)"
      ],
      "metadata": {
        "id": "P5Tfc8XAxkvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "metric, diffs = test_resnet(model, test_d, device)\n",
        "print('res= ', metric)\n",
        "print(sorted(diffs)[0:5])\n",
        "print(sorted(diffs)[-5:])"
      ],
      "metadata": {
        "id": "hJ0WzkIUxm4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Uoe0EQeVHU"
      },
      "source": [
        "**Plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywRRzm8xeU4H"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(train_loss_list[1:], label='train', color='maroon')\n",
        "ax.plot(val_loss_list[1:], label='validation', color='green')\n",
        "\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('loss')\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}