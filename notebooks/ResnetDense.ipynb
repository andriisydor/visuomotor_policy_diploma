{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7kKhuqUEQjD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X-Jx4cNUu9I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVr-eyaSSCyF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuF4VE76SOo9"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/diploma'\n",
        "DATASET_DIR = '/dataset-v3'\n",
        "IMGS_DIR = '/imgs'\n",
        "IMGS_PATH = DATA_PATH + DATASET_DIR + IMGS_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxkPTzwHpH-h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_row_v2(row):\n",
        "  devided = row.split(\" --&#!--- \")\n",
        "  target_part = devided[-1]\n",
        "  target = json.loads(target_part)\n",
        "  return target\n",
        "\n",
        "\n",
        "def parse_actions_file(rows):\n",
        "  actions = []\n",
        "  for row in rows:\n",
        "    if not \" --&#!--- \" in row:\n",
        "      continue\n",
        "    actions.append(parse_row_v2(row))\n",
        "  return actions\n",
        "\n",
        "\n",
        "def reformat_actions_into_dict(parsed_rows):\n",
        "  result = {}\n",
        "  for row in parsed_rows:\n",
        "    result[row['t']] = row\n",
        "  return result\n",
        "\n",
        "\n",
        "def episode_actions_dict(episode_number):\n",
        "  with open(DATA_PATH + DATASET_DIR + \"/\" + str(episode_number), \"r\") as f:\n",
        "    raw_data = f.readlines()\n",
        "  parsed_actions = parse_actions_file(raw_data)\n",
        "  actions_dict = reformat_actions_into_dict(parsed_actions)\n",
        "  return actions_dict\n",
        "\n",
        "\n",
        "def episode_images_list(episode_number):\n",
        "  return os.listdir(IMGS_PATH + str(episode_number))\n",
        "\n",
        "\n",
        "def merge_actions_and_images(actions, images_list, images_path):\n",
        "  SIGNS_TO_COMPARE = 11\n",
        "  def to_short_time(t):\n",
        "    return int(str(t)[:SIGNS_TO_COMPARE])\n",
        "\n",
        "  result = {}\n",
        "  actions_count = len(actions)\n",
        "  images_count = len(images_list)\n",
        "  images_short = {}\n",
        "\n",
        "  for i in images_list:\n",
        "    key = to_short_time(i)\n",
        "    if not key in images_short:\n",
        "      images_short[key] = i\n",
        "\n",
        "  for action_time in actions.keys():\n",
        "    short_time = to_short_time(action_time)\n",
        "    if (short_time in images_short):\n",
        "      the_item = actions[action_time]\n",
        "      the_item['img'] = images_path + '/' + images_short[short_time]\n",
        "      result[short_time] = the_item\n",
        "  return result\n",
        "\n",
        "\n",
        "def reduce_frequency_by_step(episode_data, step):\n",
        "  new_episode_data = {}\n",
        "  sorted_keys = sorted(episode_data.keys())\n",
        "  for i in range(step, len(sorted_keys), 1):\n",
        "    new_elem = episode_data[sorted_keys[i-step]].copy()\n",
        "    next_elem = episode_data[sorted_keys[i]]\n",
        "    new_elem['t'] = next_elem['t']\n",
        "    new_elem['j'] = next_elem['j']\n",
        "    new_episode_data[sorted_keys[i - step]] = new_elem\n",
        "  for i in range(-step, 0, 1):\n",
        "    new_elem = episode_data[sorted_keys[i]].copy()\n",
        "    next_elem = episode_data[sorted_keys[-1]]\n",
        "    new_elem['t'] = next_elem['t']\n",
        "    new_elem['j'] = next_elem['j']\n",
        "    new_episode_data[sorted_keys[i]] = new_elem\n",
        "  return new_episode_data\n",
        "\n",
        "\n",
        "def with_action_horizon(reduced, action_horizon):\n",
        "  reduced_keys = sorted(list(reduced.keys()))\n",
        "  len_reduced = len(reduced)\n",
        "  result = {}\n",
        "\n",
        "  for i in range(len_reduced):\n",
        "    to_append_to_current = []\n",
        "    current = reduced[reduced_keys[i]]['j'].copy()\n",
        "\n",
        "    for j in range(1, action_horizon):\n",
        "      to_append = current.copy()\n",
        "      if (i + j) < len_reduced:\n",
        "        to_append = reduced[reduced_keys[i + j]]['j'].copy()\n",
        "\n",
        "      to_append_to_current += to_append.copy()\n",
        "\n",
        "    result[reduced_keys[i]] = {\n",
        "        'j': reduced[reduced_keys[i]]['j'].copy() + to_append_to_current.copy(),\n",
        "        'current': reduced[reduced_keys[i]]['current'].copy(),\n",
        "        't': reduced[reduced_keys[i]]['t'],\n",
        "        'img': reduced[reduced_keys[i]]['img']\n",
        "        }\n",
        "  return result\n",
        "\n",
        "def form_data_for_episode(episode_number, reduction_step, action_horizon):\n",
        "  actions_dict = episode_actions_dict(episode_number)\n",
        "  images_list = episode_images_list(episode_number)\n",
        "  episode_data = merge_actions_and_images(actions_dict, sorted(images_list), IMGS_PATH + str(episode_number))\n",
        "  reduced_data = reduce_frequency_by_step(episode_data, reduction_step)\n",
        "  data_with_action_horizon = with_action_horizon(reduced_data, action_horizon)\n",
        "  return data_with_action_horizon\n",
        "\n",
        "\n",
        "def episodes_data(numbers_of_episodes, reduction_step, action_horizon):\n",
        "  result = {}\n",
        "  for i in numbers_of_episodes:\n",
        "    result.update(form_data_for_episode(i, reduction_step, action_horizon))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSeAjXAmYja-"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data, image_transform, action_transform):\n",
        "        self.data = list(data.values())\n",
        "        self.image_transform = image_transform\n",
        "        self.action_transform = action_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        image = Image.open(sample['img'])\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        action = self.action_transform(sample['j'])\n",
        "        current = self.action_transform(sample['current'])\n",
        "        return image, torch.tensor(action), torch.tensor(current)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenceNN(nn.Module):\n",
        "  def __init__(self, action_horizon):\n",
        "    super().__init__()\n",
        "    self.dence_layer_1 = nn.Sequential(\n",
        "        nn.Linear(1128, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.dence_head = nn.Sequential(\n",
        "        nn.Linear(64, action_horizon * 7)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    result = self.dence_layer_1(x)\n",
        "    result = self.dence_head(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "class ResnetVisiomotorPolicy(nn.Module):\n",
        "  def __init__(self, action_horizon, pretrained_resnet):\n",
        "    super().__init__()\n",
        "\n",
        "    feature_exctractor = None\n",
        "    if pretrained_resnet:\n",
        "      feature_exctractor = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights, progress=False)\n",
        "    else:\n",
        "      feature_exctractor = torchvision.models.resnet34(progress=False)\n",
        "\n",
        "    # if freeze_resnet:\n",
        "    #   for param in feature_exctractor.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "    assert(feature_exctractor)\n",
        "    self.perception_network = feature_exctractor\n",
        "    self.policy_network = DenceNN(action_horizon)\n",
        "    self.joint_space_projection = nn.Linear(7, 128)\n",
        "\n",
        "  def forward(self, image, current_state):\n",
        "    visual_repr = self.perception_network(image)\n",
        "    joints_repr = self.joint_space_projection(current_state)\n",
        "    concat_repr = torch.cat((visual_repr, joints_repr), -1)\n",
        "    return self.policy_network(concat_repr)"
      ],
      "metadata": {
        "id": "jtSHkN4oNPz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_visuomotor_policy(\n",
        "    model, dataset, learning_rate, batch_size, num_of_epochs, device, shuffle,\n",
        "    validation_set, loss_list, val_loss_list):\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.L1Loss(reduction=\"mean\")\n",
        "  dataloader = torch.utils.data.DataLoader(\n",
        "      dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "  validation_loader = torch.utils.data.DataLoader(\n",
        "      dataset=validation_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(num_of_epochs):\n",
        "    loss_sum = 0\n",
        "    batches_count = 0\n",
        "    for data in dataloader:\n",
        "      batches_count += 1\n",
        "      images, labels, currents = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      currents = currents.to(device)\n",
        "\n",
        "      prediction = model(images, currents)\n",
        "\n",
        "      loss = criterion(prediction, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_sum += loss.item()\n",
        "\n",
        "    epoch_loss = loss_sum / batches_count\n",
        "    loss_list.append(epoch_loss)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = 0\n",
        "    with torch.no_grad():\n",
        "      val_criterion = nn.L1Loss(reduction=\"mean\")\n",
        "      val_batches_count = 0\n",
        "      for val_data in validation_loader:\n",
        "        val_batches_count += 1\n",
        "        v_images, v_labels, v_currents = val_data\n",
        "        v_images = v_images.to(device)\n",
        "        v_labels = v_labels.to(device)\n",
        "        v_currents = v_currents.to(device)\n",
        "\n",
        "        v_prediction = model(v_images, v_currents)\n",
        "\n",
        "        v_loss += val_criterion(v_prediction, v_labels).item()\n",
        "\n",
        "      val_loss = v_loss / val_batches_count\n",
        "      val_loss_list.append(val_loss)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print(f'epoch \\t{epoch}\\t -- train loss: {epoch_loss} -- val loss: {val_loss}')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "VSbyJhH_Jy5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqcEU1uvvx7t"
      },
      "outputs": [],
      "source": [
        "image_transformator = transforms.Compose([\n",
        "    # transforms.Grayscale(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.ColorJitter(0.4, 0.4, 0.0, 0.3),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_image_transformator = transforms.Compose([\n",
        "    # transforms.Grayscale(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def action_transformator(action):\n",
        "  return action\n",
        "  # result = []\n",
        "  # for i in range(len(action)):\n",
        "  #   result.append((action[i] - (-6.284)) / (6.284 - (-6.284)))\n",
        "  # return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION_HORIZON = 1\n",
        "FREQUENCY_REDUCTION = 25"
      ],
      "metadata": {
        "id": "I57GM27ir3x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD3dT8BU3E9k"
      },
      "outputs": [],
      "source": [
        "train_data = episodes_data([21, 23, 24, 25, 26, 27, 30, 31, 32], FREQUENCY_REDUCTION, ACTION_HORIZON)\n",
        "val_data = episodes_data([28, 29], FREQUENCY_REDUCTION, ACTION_HORIZON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYadv_9HaDSY"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomImageDataset(train_data, image_transformator, action_transformator)\n",
        "val_dataset = CustomImageDataset(val_data, test_image_transformator, action_transformator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEbZG1CYqo0_"
      },
      "outputs": [],
      "source": [
        "model = ResnetVisiomotorPolicy(ACTION_HORIZON, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx3Yk44AbOK8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm0Dk5GJC4vh"
      },
      "outputs": [],
      "source": [
        "train_loss_list = []\n",
        "val_loss_list = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_visuomotor_policy(model, train_dataset, 0.00001,\n",
        "                                batch_size=8, num_of_epochs=50,\n",
        "                                device=device, shuffle=True,\n",
        "                                validation_set=val_dataset,\n",
        "                                loss_list=train_loss_list,\n",
        "                                val_loss_list=val_loss_list)"
      ],
      "metadata": {
        "id": "qGkS4veNTmjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_visuomotor_policy(model, train_dataset, 0.0000001,\n",
        "                                     batch_size=8, num_of_epochs=10,\n",
        "                                     device=device, shuffle=True,\n",
        "                                     validation_set=val_dataset,\n",
        "                                     loss_list=train_loss_list,\n",
        "                                     val_loss_list=val_loss_list)"
      ],
      "metadata": {
        "id": "cNVRJbfrFAD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_visuomotor_policy(model, train_dataset, 0.000000001,\n",
        "                                     batch_size=8, num_of_epochs=10,\n",
        "                                     device=device, shuffle=True,\n",
        "                                     validation_set=val_dataset,\n",
        "                                     loss_list=train_loss_list,\n",
        "                                     val_loss_list=val_loss_list)"
      ],
      "metadata": {
        "id": "DB7A6H_IIe5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SPJBtLVf_YY"
      },
      "outputs": [],
      "source": [
        "TRAINED_MODEL = '/comparation_resnet_model_v1_1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJBr6EGfOxcw"
      },
      "outputs": [],
      "source": [
        "torch.save(model, DATA_PATH + '/models' + TRAINED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYtXGWc99aSh"
      },
      "source": [
        "**Model testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziJeESlU992j"
      },
      "outputs": [],
      "source": [
        "def test_resnet(model, dataset, device):\n",
        "  criterion = nn.L1Loss(reduction=\"mean\")\n",
        "  dataloader = torch.utils.data.DataLoader(\n",
        "      dataset=dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "  diffs = []\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.eval()\n",
        "  loss_sum = 0\n",
        "  batches_num = 0\n",
        "  for data in dataloader:\n",
        "    batches_num += 1\n",
        "    images, labels, currents = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    currents = currents.to(device)\n",
        "\n",
        "    prediction = model(images, currents)\n",
        "\n",
        "    loss = criterion(prediction, labels)\n",
        "\n",
        "    diff = torch.abs(prediction - labels)\n",
        "    diffs.append(diff.max().item())\n",
        "\n",
        "    loss_sum += loss.item()\n",
        "  epoch_loss = loss_sum / batches_num\n",
        "  return epoch_loss, diffs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(DATA_PATH + '/models' + '/comparation_resnet_model_v1_1', map_location=device)"
      ],
      "metadata": {
        "id": "JhHm-5QLrGS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "sQHLLNHHU3aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDmg9Aj4CEmk"
      },
      "outputs": [],
      "source": [
        "test_data = episodes_data([19, 22], FREQUENCY_REDUCTION, ACTION_HORIZON)\n",
        "test_d = CustomImageDataset(test_data, test_image_transformator, action_transformator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "metric, diffs = test_resnet(model, test_d, device)\n",
        "print('res= ', metric)\n",
        "print(sorted(diffs)[0:5])\n",
        "print(sorted(diffs)[-5:])"
      ],
      "metadata": {
        "id": "RDscrlRgXGIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = episodes_data([22], FREQUENCY_REDUCTION, ACTION_HORIZON)\n",
        "test_d = CustomImageDataset(test_data, test_image_transformator, action_transformator)"
      ],
      "metadata": {
        "id": "P5Tfc8XAxkvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "metric, diffs = test_resnet(model, test_d, device)\n",
        "print('res= ', metric)\n",
        "print(sorted(diffs)[0:5])\n",
        "print(sorted(diffs)[-5:])"
      ],
      "metadata": {
        "id": "hJ0WzkIUxm4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Uoe0EQeVHU"
      },
      "source": [
        "**Plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywRRzm8xeU4H"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(train_loss_list[1:], label='train', color='maroon')\n",
        "ax.plot(val_loss_list[1:], label='validation', color='green')\n",
        "\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('loss')\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}